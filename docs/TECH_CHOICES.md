### Технологические выборы (OCR, очередь, LLM)

Этот документ фиксирует принятые решения для базовой конфигурации сервиса и служит ссылкой при реализации и отладке. Для деталей общего плана см. `docs/PHASED_PLAN.md`.

## OCR
- Основной: PaddleOCR (GPU)
- Фолбэк: Tesseract (CPU)

Обоснование:
- Для топ‑10 распространённых языков (латиница, кириллица, китайский, арабский, деванагари/хинди, бенгальский и др.) PaddleOCR на GPU обеспечивает лучшее качество и скорость, особенно на CJK и арабице.
- Tesseract остаётся простым и надёжным фолбэком без GPU и для сред с ограниченными зависимостями.

Практика применения:
- Детектировать наличие текстового слоя перед OCR и пропускать OCR при полноценных текстовых PDF.
- Дефолт — PaddleOCR GPU; при недоступности GPU или ошибке модуля падать на Tesseract CPU.
- Поддерживать выбор языка/наборов языков через конфигурацию.

Чеклист OCR:
- [ ] NVIDIA runtime доступен в контейнере, GPU видна процессу
- [ ] Модели/языковые пакеты скачиваются оффлайн и кешируются (`HF_HOME`, `TORCH_HOME`, каталоги Paddle)
- [ ] Детектор текстового слоя до OCR включён
- [ ] Конфиг фолбэка: PaddleOCR → Tesseract при ошибке/таймауте/отсутствии GPU
- [ ] Метрики по странице: длительность, агент OCR, язык, confidence

## Очередь/оркестрация
- Выбор: Celery + Redis

Обоснование:
- Celery предоставляет зрелые механизмы ретраев, таймаутов, цепочек, роутинга задач и периодических задач (через beat). Redis прост в эксплуатации и достаточен для нашего профиля.

Рекомендованные очереди и конкурентность:
- Очереди: `cpu`, `ocr`, `caption`, `donut`, `llm`
- Лимит параллелизма для GPU‑тяжёлых очередей (`ocr`, `caption`, `llm`) через пулы/семафоры
- Идемпотентность задач и ретраи с джиттером

Чеклист очереди:
- [ ] Celery настроен с Redis (URI, пулы, prefetch)
- [ ] Раздельные очереди по шагам пайплайна и роутинг из API
- [ ] Политики ретраев, дедупликация, идемпотентные работы
- [ ] Планировщик TTL‑клинера (Celery beat) включён
- [ ] Корреляционные ID в логах от API до воркеров

## LLM рантайм
- Основной для прод: vLLM (GPU)
- Фолбэк для оффлайн/CPU: возможно `llama.cpp/gguf` (опционально)

Обоснование:
- vLLM даёт высокий throughput, длинный контекст и экономию VRAM (PagedAttention), хорошо интегрируется с моделями уровня Qwen2.5‑7B‑Instruct.
- Для локального запуска без GPU можно использовать квантованные gguf через llama.cpp; для PoC допустим Ollama, но в проде предпочтительнее прямое управление рантаймом.

Практика применения:
- В прод‑профиле загружать модель в vLLM; включить строгий JSON‑вывод/гайды и ретраи при невалидном JSON
- Вести учёт токенов (prompt/response), ограничивать контекст и размер вывода

Чеклист LLM:
- [ ] vLLM развёрнут и доступен из воркеров (локально или по сети)
- [ ] Квантованные/FP16 веса подготовлены оффлайн; кэш моделей смонтирован
- [ ] Промпты для суммари/страничного Q&A/структурированного вывода протестированы
- [ ] Guardrails: лимиты контекста, таймауты, повтор на невалидный JSON
- [ ] Метрики токенов, времени, числа ретраев

## Профили развёртывания
- GPU‑профиль (прод): PaddleOCR GPU, Celery+Redis, vLLM; лимиты на параллелизм GPU‑шагов
- CPU‑профиль (fallback/локально): Tesseract, Celery+Redis, при необходимости llama.cpp/gguf

Ссылки:
- План фаз: `docs/PHASED_PLAN.md`
